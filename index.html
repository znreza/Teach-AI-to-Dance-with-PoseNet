<html>
<meta charset="utf-8"/>
 <head>
  	<script src = "https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.11.6"> </script>
  	<script src="https://cdn.jsdelivr.net/npm/dat.gui@0.7.2/build/dat.gui.js"></script>
  	<script src="https://unpkg.com/@tensorflow-models/posenet"></script>
  	<script src="demo_util.js"></script>
  	
  	<script src="https://cdn.webrtc-experiment.com/RecordRTC/Whammy.js"></script>
    <script src="https://cdn.webrtc-experiment.com/RecordRTC/CanvasRecorder.js"></script>
    <script src = "https://cdn.webrtc-experiment.com/screenshot.js"></script>
    <script src="https://cdn.webrtc-experiment.com/RecordRTC.js"></script>
    
</head>

<body>

	<div id="main"> 

  <video id="video" src= "vid1.mp4" crossOrigin = "Anonymous" type="video/mp4">
    
</video>
<br>
    <canvas id="output" </canvas>
    <br>

</div>

<div>
        <button type="button" id="recording">Start Recording</button>
        <button type="button" id="play" disabled>Play</button>
        <button type="button" id="download" disabled>Download</button>
    </div>

<script type="module">

function isAndroid() {
  return /Android/i.test(navigator.userAgent);
}

function isiOS() {
  return /iPhone|iPad|iPod/i.test(navigator.userAgent);
}

function isMobile() {
  return isAndroid() || isiOS();
}

 const videoWidth = 600;
 const videoHeight = 500;

async function setupCamera() {
		

    const video = document.getElementById('video');
    video.width = videoWidth;
    video.height = videoHeight;
    video.play();

    return new Promise((resolve) => {
        resolve(video);
    });
		}

async function loadVideo() {
	const video = await setupCamera();
	
	return video;
		}

const guiState = {
	algorithm: 'multi-pose',
	input: {
	mobileNetArchitecture: 0.50,
	outputStride: 16,
	imageScaleFactor: 0.5,
	},
	singlePoseDetection: {
	minPoseConfidence: 0.1,
	minPartConfidence: 0.5,
	},
	multiPoseDetection: {
	maxPoseDetections: 3,
	minPoseConfidence: 0.5,
	minPartConfidence: 0.5,
	nmsRadius: 30.0,
	},
	output: {
	showVideo: true,
	showSkeleton: true,
	showPoints: true,
	},
	net: null,
};
	    
  function setupGui( cameras, net) {
  guiState.net = net;

  const gui = new dat.GUI({width: 300});

  const algorithmController =
      gui.add(guiState, 'algorithm', ['single-pose', 'multi-pose']);

  let input = gui.addFolder('Input');
  // Architecture: there are a few PoseNet models varying in size and
  // accuracy. 1.01 is the largest, but will be the slowest. 0.50 is the
  // fastest, but least accurate.
  const architectureController = input.add(
      guiState.input, 'mobileNetArchitecture',
      ['1.01', '1.00', '0.75', '0.50']);
 
  input.add(guiState.input, 'outputStride', [8, 16, 32]);
  // Image scale factor: What to scale the image by before feeding it through
  // the network.
  input.add(guiState.input, 'imageScaleFactor').min(0.2).max(1.0);
  input.open();


  let single = gui.addFolder('Single Pose Detection');
  single.add(guiState.singlePoseDetection, 'minPoseConfidence', 0.0, 1.0);
  single.add(guiState.singlePoseDetection, 'minPartConfidence', 0.0, 1.0);

  let multi = gui.addFolder('Multi Pose Detection');
  multi.add(guiState.multiPoseDetection, 'maxPoseDetections')
      .min(1)
      .max(20)
      .step(1);
  multi.add(guiState.multiPoseDetection, 'minPoseConfidence', 0.0, 1.0);
  multi.add(guiState.multiPoseDetection, 'minPartConfidence', 0.0, 1.0);
  // nms Radius: controls the minimum distance between poses that are returned
  // defaults to 20, which is probably fine for most use cases
  multi.add(guiState.multiPoseDetection, 'nmsRadius').min(0.0).max(40.0);
  multi.open();

  let output = gui.addFolder('Output');
  output.add(guiState.output, 'showVideo');
  output.add(guiState.output, 'showSkeleton');
  output.add(guiState.output, 'showPoints');
  output.open();

  architectureController.onChange(function(architecture) {
    guiState.changeToArchitecture = architecture;
  });

  algorithmController.onChange(function(value) {
    switch (guiState.algorithm) {
      case 'single-pose':
        multi.close();
        single.open();
        break;
      case 'multi-pose':
        single.close();
        multi.open();
        break;
    }
  });
}

function detectPoseInRealTime(video, net) {

  const canvas = document.getElementById('output');
  const ctx = canvas.getContext('2d');
  // since images are being fed from a webcam
  const flipHorizontal = true;

  canvas.width = videoWidth;
  canvas.height = videoHeight;
  
  async function poseDetectionFrame() {
    if (guiState.changeToArchitecture) {
      // Important to purge variables and free up GPU memory
      guiState.net.dispose();

      // Load the PoseNet model weights for either the 0.50, 0.75, 1.00, or 1.01
      // version
      guiState.net = await posenet.load(+guiState.changeToArchitecture);

      guiState.changeToArchitecture = null;
    }

    const imageScaleFactor = guiState.input.imageScaleFactor;
    const outputStride = +guiState.input.outputStride;

    let poses = [];
    let minPoseConfidence;
    let minPartConfidence;
    switch (guiState.algorithm) {
      case 'single-pose':
        const pose = await guiState.net.estimateSinglePose(
            video, imageScaleFactor, flipHorizontal, outputStride);
        poses.push(pose);

        minPoseConfidence = +guiState.singlePoseDetection.minPoseConfidence;
        minPartConfidence = +guiState.singlePoseDetection.minPartConfidence;
        break;
      case 'multi-pose':
 
  	    poses = await guiState.net.estimateMultiplePoses(
        video, imageScaleFactor, flipHorizontal, outputStride,
        guiState.multiPoseDetection.maxPoseDetections,
        guiState.multiPoseDetection.minPartConfidence,
        guiState.multiPoseDetection.nmsRadius);

        minPoseConfidence = +guiState.multiPoseDetection.minPoseConfidence;
        minPartConfidence = +guiState.multiPoseDetection.minPartConfidence;
        
      break;
    }
    
    ctx.clearRect(0, 0, videoWidth, videoHeight);

    if (guiState.output.showVideo) {
      ctx.save();
      ctx.scale(-1, 1);
      ctx.translate(-videoWidth, 0);
      ctx.drawImage(video, 0, 0, videoWidth, videoHeight);
      ctx.restore();
    }

    poses.forEach(({score, keypoints}) => {
      if (score >= minPoseConfidence) {
        if (guiState.output.showPoints) {
          drawKeypoints(keypoints, minPartConfidence, ctx);
        }
        if (guiState.output.showSkeleton) {
          drawSkeleton(keypoints, minPartConfidence, ctx);
        }
      }
    });

    requestAnimationFrame(poseDetectionFrame);
    
  }
  poseDetectionFrame();
  
}

export async function bindPage() {
  const net = await posenet.load(0.75);

  let video;
    video = await loadVideo();

  setupGui([], net);
  detectPoseInRealTime(video, net);
  
}

const button = document.querySelector('button#recording');
button.addEventListener('click', function(e) {
  bindPage();
});


</script>
<script src="canvasRecord.js"></script>
	   
</body>
</html>